{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52da2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atlanta/miniconda3/envs/Custom_train/lib/python3.10/site-packages/torch/serialization.py:1488: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ The file 'yolopv2.pt' appears to be improperly saved or formatted. For optimal results, use model.save('filename.pt') to correctly save YOLO models.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RecursiveScriptModule' object has no attribute 'fuse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m img_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# === Inference ===\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_rgb\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Inference on single image\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# === Alpha blending setup ===\u001b[39;00m\n\u001b[1;32m     25\u001b[0m overlay \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/Custom_train/lib/python3.10/site-packages/ultralytics/engine/model.py:185\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    158\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    161\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Custom_train/lib/python3.10/site-packages/ultralytics/engine/model.py:548\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[0;32m~/miniconda3/envs/Custom_train/lib/python3.10/site-packages/ultralytics/engine/predictor.py:391\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[0;34m(self, model, verbose)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    Initialize YOLO model with given parameters and set it to evaluation mode.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m        verbose (bool): Whether to print verbose output.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Custom_train/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Custom_train/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:199\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, fuse, verbose)\u001b[0m\n\u001b[1;32m    197\u001b[0m model \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[0;32m--> 199\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m(verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    201\u001b[0m     kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Custom_train/lib/python3.10/site-packages/torch/jit/_script.py:829\u001b[0m, in \u001b[0;36mRecursiveScriptModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[attr] \u001b[38;5;241m=\u001b[39m script_method\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m script_method\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Custom_train/lib/python3.10/site-packages/torch/jit/_script.py:536\u001b[0m, in \u001b[0;36mScriptModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_actual_script_module\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m--> 536\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actual_script_module, attr)\n",
      "File \u001b[0;32m~/miniconda3/envs/Custom_train/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RecursiveScriptModule' object has no attribute 'fuse'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# === CONFIG ===\n",
    "model_path = \"runs/segment/Lane_V3s2/weights/best.pt\"  # Path to your trained model\n",
    "image_path = \"test_images/multi.png\"           # Path to test image\n",
    "output_path = \"output/demo_1.jpeg\"    # Save path\n",
    "\n",
    "# === Load model ===\n",
    "model = YOLO(model_path)  # Load your YOLOv8/v11 segmentation model\n",
    "\n",
    "# === Load image ===\n",
    "img = cv2.imread(image_path)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# === Inference ===\n",
    "results = model(img_rgb)[0]  # Inference on single image\n",
    "\n",
    "# === Alpha blending setup ===\n",
    "overlay = img.copy()\n",
    "alpha = 0.5  # Transparency of mask overlays\n",
    "mask_color = (0, 255, 0)  # Green mask\n",
    "\n",
    "# === Resize and blend masks ===\n",
    "if results.masks is not None:\n",
    "    masks = results.masks.data.cpu().numpy()  # Shape: [N, H_mask, W_mask]\n",
    "    orig_h, orig_w = img.shape[:2]\n",
    "\n",
    "    for mask in masks:\n",
    "        # Resize the mask to the original image size\n",
    "        mask_resized = cv2.resize((mask * 255).astype(np.uint8), (orig_w, orig_h))\n",
    "\n",
    "        # Create a color mask\n",
    "        color_mask = np.zeros_like(img, dtype=np.uint8)\n",
    "        color_mask[:, :, 1] = mask_resized  # Green channel\n",
    "\n",
    "        # Blend with original image\n",
    "        overlay = cv2.addWeighted(overlay, 1, color_mask, alpha, 0)\n",
    "\n",
    "# === Final result ===\n",
    "img_result = overlay\n",
    "\n",
    "# === Save result ===\n",
    "os.makedirs(Path(output_path).parent, exist_ok=True)\n",
    "cv2.imwrite(output_path, img_result)\n",
    "print(f\"✓ Result saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd950eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 lanes, 5.1ms\n",
      "Speed: 2.5ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 1.8ms preprocess, 4.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.6ms\n",
      "Speed: 1.2ms preprocess, 3.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.1ms preprocess, 4.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.2ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.6ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 2.4ms preprocess, 4.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.4ms preprocess, 4.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 1.6ms preprocess, 4.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.0ms\n",
      "Speed: 2.5ms preprocess, 5.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.5ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.1ms\n",
      "Speed: 2.2ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 2.8ms preprocess, 4.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.1ms preprocess, 3.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 2.2ms preprocess, 4.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 1.7ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 2.7ms preprocess, 4.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 1.5ms preprocess, 3.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.1ms preprocess, 3.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 1.6ms preprocess, 3.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 1.4ms preprocess, 3.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.3ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.6ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.3ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 2.3ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 1.7ms preprocess, 4.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.0ms\n",
      "Speed: 2.6ms preprocess, 5.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 1.9ms preprocess, 4.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.6ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 2.9ms preprocess, 4.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 1.4ms preprocess, 4.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 2.2ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.9ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 1.5ms preprocess, 3.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 2.8ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.9ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 2.9ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.4ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.2ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 1.5ms preprocess, 4.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.3ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 2.3ms preprocess, 4.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.7ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 1.8ms preprocess, 3.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.1ms preprocess, 3.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.2ms preprocess, 4.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 2.2ms preprocess, 3.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.3ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 11.7ms\n",
      "Speed: 3.0ms preprocess, 11.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 2.9ms preprocess, 4.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.7ms preprocess, 4.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 1.4ms preprocess, 4.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 2.1ms preprocess, 4.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.2ms preprocess, 3.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.4ms preprocess, 4.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 1.8ms preprocess, 3.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.1ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 1.7ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.2ms preprocess, 4.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.3ms preprocess, 4.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 2.7ms preprocess, 4.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 2.2ms preprocess, 4.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.0ms preprocess, 3.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.2ms\n",
      "Speed: 2.2ms preprocess, 5.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.1ms preprocess, 4.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.1ms preprocess, 3.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 1.5ms preprocess, 3.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 1.8ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.3ms preprocess, 4.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.3ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 2.5ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 1.8ms preprocess, 4.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.3ms\n",
      "Speed: 2.1ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.7ms preprocess, 3.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.1ms preprocess, 3.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 1.7ms preprocess, 3.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.7ms preprocess, 3.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.0ms preprocess, 3.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.3ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.0ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.3ms preprocess, 4.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.4ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 1.6ms preprocess, 4.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 3.0ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 1.9ms preprocess, 4.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.4ms preprocess, 4.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.2ms preprocess, 4.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.1ms preprocess, 3.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 1.5ms preprocess, 3.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.7ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 1.6ms preprocess, 4.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.2ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.2ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.4ms preprocess, 4.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.9ms\n",
      "Speed: 2.6ms preprocess, 4.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.5ms preprocess, 4.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 1.8ms preprocess, 4.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.1ms preprocess, 4.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 2.2ms preprocess, 4.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.5ms preprocess, 4.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 2.2ms preprocess, 3.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.7ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.3ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.7ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 1.6ms preprocess, 4.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.8ms preprocess, 4.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.1ms preprocess, 4.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.7ms preprocess, 4.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.8ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.4ms preprocess, 4.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 2.2ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.5ms\n",
      "Speed: 1.7ms preprocess, 3.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.1ms preprocess, 3.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 2.5ms preprocess, 4.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 2.4ms preprocess, 4.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 1.6ms preprocess, 4.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 2.7ms preprocess, 4.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 1.6ms preprocess, 3.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.8ms preprocess, 4.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.9ms preprocess, 3.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.4ms preprocess, 4.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.1ms preprocess, 4.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 2.1ms preprocess, 4.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.0ms preprocess, 4.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.9ms preprocess, 3.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.0ms preprocess, 3.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 2.8ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.0ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 1.6ms preprocess, 4.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.9ms\n",
      "Speed: 2.3ms preprocess, 4.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.2ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.8ms preprocess, 4.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.8ms preprocess, 4.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.4ms preprocess, 4.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.1ms preprocess, 4.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.1ms preprocess, 3.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.6ms\n",
      "Speed: 1.5ms preprocess, 3.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 1.7ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.3ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.0ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 8.1ms\n",
      "Speed: 2.5ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.8ms preprocess, 4.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.3ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 1.6ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 2.5ms preprocess, 4.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.5ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 1.3ms preprocess, 4.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.1ms preprocess, 4.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 1.5ms preprocess, 4.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.0ms\n",
      "Speed: 2.1ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.0ms preprocess, 4.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.8ms preprocess, 4.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.3ms preprocess, 4.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 1.5ms preprocess, 4.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 2.2ms preprocess, 4.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.2ms preprocess, 4.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.1ms preprocess, 3.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 1.5ms preprocess, 4.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.2ms preprocess, 4.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.2ms preprocess, 4.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 1.6ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.4ms preprocess, 4.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 1.8ms preprocess, 4.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.3ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 lanes, 4.1ms\n",
      "Speed: 2.3ms preprocess, 4.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 2.7ms preprocess, 4.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.9ms preprocess, 3.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 2.1ms preprocess, 4.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.8ms\n",
      "Speed: 2.1ms preprocess, 4.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.5ms preprocess, 4.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.0ms\n",
      "Speed: 2.2ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.6ms\n",
      "Speed: 1.8ms preprocess, 4.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.0ms preprocess, 3.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.1ms preprocess, 3.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.5ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.7ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 2.3ms preprocess, 4.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.7ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.3ms preprocess, 4.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.4ms preprocess, 3.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 2.1ms preprocess, 4.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 3.1ms preprocess, 4.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 1.4ms preprocess, 4.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.2ms preprocess, 4.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.6ms\n",
      "Speed: 1.7ms preprocess, 3.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.1ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.2ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 1.5ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.3ms preprocess, 4.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.2ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.7ms\n",
      "Speed: 2.2ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 5.0ms\n",
      "Speed: 2.7ms preprocess, 5.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 1.6ms preprocess, 4.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.9ms\n",
      "Speed: 2.4ms preprocess, 4.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.2ms\n",
      "Speed: 2.4ms preprocess, 4.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.3ms\n",
      "Speed: 2.2ms preprocess, 4.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.3ms preprocess, 3.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.8ms preprocess, 3.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.7ms preprocess, 4.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.7ms preprocess, 4.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.8ms preprocess, 4.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 1.5ms preprocess, 4.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 1.7ms preprocess, 3.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 2.0ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.2ms preprocess, 4.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.4ms\n",
      "Speed: 2.4ms preprocess, 4.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.0ms\n",
      "Speed: 2.2ms preprocess, 4.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.9ms\n",
      "Speed: 1.6ms preprocess, 3.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.7ms preprocess, 4.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.1ms\n",
      "Speed: 2.7ms preprocess, 4.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 4.5ms\n",
      "Speed: 1.8ms preprocess, 4.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.7ms\n",
      "Speed: 1.5ms preprocess, 3.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 lanes, 3.8ms\n",
      "Speed: 2.2ms preprocess, 3.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "✓ Video result saved to: output/demo_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# === CONFIG ===\n",
    "model_path = \"runs/segment/Lane_V3s2/weights/best.pt\"   # Path to trained YOLO model\n",
    "video_path = \"test_video/test.mp4\"                   # Path to test video\n",
    "output_path = \"output/demo_video.mp4\"                 # Output video path\n",
    "\n",
    "# === Load YOLO model ===\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# === Open video ===\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Video not found at {video_path}\")\n",
    "\n",
    "# Video writer setup\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "os.makedirs(Path(output_path).parent, exist_ok=True)\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# === Process video frames ===\n",
    "alpha = 0.5  # Mask transparency\n",
    "mask_color = (0, 255, 0)  # Green mask\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Inference\n",
    "    results = model(frame_rgb)[0]\n",
    "\n",
    "    overlay = frame.copy()\n",
    "    if results.masks is not None:\n",
    "        masks = results.masks.data.cpu().numpy()\n",
    "        for mask in masks:\n",
    "            mask_resized = cv2.resize((mask * 255).astype(np.uint8), (width, height))\n",
    "            color_mask = np.zeros_like(frame, dtype=np.uint8)\n",
    "            color_mask[:, :, 1] = mask_resized  # Green channel\n",
    "            overlay = cv2.addWeighted(overlay, 1, color_mask, alpha, 0)\n",
    "\n",
    "    # Write frame to output video\n",
    "    out.write(overlay)\n",
    "\n",
    "# === Cleanup ===\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"✓ Video result saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a0e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model returned 3 outputs\n",
      "Output[0]: <class 'tuple'>\n",
      "Output[1]: shape=(1, 2, 384, 640), dtype=torch.float32\n",
      "Output[2]: shape=(1, 1, 384, 640), dtype=torch.float32\n",
      "✓ Result saved to /home/atlanta/Downloads/Lane_Training/yolopv2_result.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# === Device selection ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load TorchScript model ===\n",
    "model = torch.jit.load(\"yolopv2.pt\", map_location=device)\n",
    "model.eval()\n",
    "\n",
    "# === Read image ===\n",
    "img_path = \"test_images/multi.png\"\n",
    "img = cv2.imread(img_path)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
    "\n",
    "# Keep original for overlay\n",
    "orig_img = img.copy()\n",
    "\n",
    "# === Resize to model's expected size ===\n",
    "target_w, target_h = 640, 384  # common YOLOPv2 size\n",
    "img_resized = cv2.resize(img, (target_w, target_h))\n",
    "img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# === Preprocess ===\n",
    "img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "# === Inference ===\n",
    "with torch.no_grad():\n",
    "    outputs = model(img_tensor)\n",
    "\n",
    "# === Debug output structure ===\n",
    "print(f\"Model returned {len(outputs) if isinstance(outputs, (tuple, list)) else 1} outputs\")\n",
    "if isinstance(outputs, (tuple, list)):\n",
    "    for i, out in enumerate(outputs):\n",
    "        if torch.is_tensor(out):\n",
    "            print(f\"Output[{i}]: shape={tuple(out.shape)}, dtype={out.dtype}\")\n",
    "        else:\n",
    "            print(f\"Output[{i}]: {type(out)}\")\n",
    "else:\n",
    "    print(f\"Single output: shape={tuple(outputs.shape)}\")\n",
    "\n",
    "# === Try to extract drivable area and lane masks ===\n",
    "def process_mask(mask_tensor, orig_shape):\n",
    "    \"\"\"Converts model mask tensor to uint8 resized mask.\"\"\"\n",
    "    if mask_tensor is None:\n",
    "        return None\n",
    "    mask = mask_tensor.squeeze().cpu().numpy()  # remove batch dim\n",
    "    if mask.ndim == 3:  # (C, H, W)\n",
    "        mask = mask[0]  # take first channel\n",
    "    mask_bin = (mask > 0.5).astype(np.uint8) * 255\n",
    "    return cv2.resize(mask_bin, (orig_shape[1], orig_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "drive_area_mask = None\n",
    "lane_line_mask = None\n",
    "\n",
    "if isinstance(outputs, (tuple, list)) and len(outputs) >= 3:\n",
    "    drive_area_mask = process_mask(outputs[1], orig_img.shape)\n",
    "    lane_line_mask = process_mask(outputs[2], orig_img.shape)\n",
    "\n",
    "# === Create overlay ===\n",
    "overlay = orig_img.copy()\n",
    "if drive_area_mask is not None:\n",
    "    overlay[drive_area_mask > 0] = (0, 255, 0)  # green\n",
    "if lane_line_mask is not None:\n",
    "    overlay[lane_line_mask > 0] = (0, 0, 255)  # red\n",
    "\n",
    "# === Blend with original ===\n",
    "alpha = 0.5\n",
    "blended = cv2.addWeighted(orig_img, 1 - alpha, overlay, alpha, 0)\n",
    "\n",
    "# === Save result ===\n",
    "output_path = Path(\"yolopv2_result.jpg\")\n",
    "cv2.imwrite(str(output_path), blended)\n",
    "print(f\"✓ Result saved to {output_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
